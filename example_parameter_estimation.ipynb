{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation\n",
    "==========\n",
    "\n",
    "An recurring statistical problem is finding estimates of the relevant parameters that correspond to the distribution that best represents our data.\n",
    "\n",
    "In **parametric** inference, we specify *a priori* a suitable distribution, then choose the parameters that best fit the data.\n",
    "\n",
    "* e.g. $\\mu$ and $\\sigma^2$ in the case of the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array([ 1.00201077,  1.58251956,  0.94515919,  6.48778002,  1.47764604,\n",
    "        5.18847071,  4.21988095,  2.85971522,  3.40044437,  3.74907745,\n",
    "        1.18065796,  3.74748775,  3.27328568,  3.19374927,  8.0726155 ,\n",
    "        0.90326139,  2.34460034,  2.14199217,  3.27446744,  3.58872357,\n",
    "        1.20611533,  2.16594393,  5.56610242,  4.66479977,  2.3573932 ])\n",
    "_ = plt.hist(x, bins=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data to probability distributions\n",
    "\n",
    "We start with the problem of finding values for the parameters that provide the best fit between the model and the data, called point estimates. First, we need to define what we mean by ‘best fit’. There are two commonly used criteria:\n",
    "\n",
    "* **Method of moments** chooses the parameters so that the sample moments (typically the sample mean and variance) match the theoretical moments of our chosen distribution.\n",
    "* **Maximum likelihood** chooses the parameters to maximize the likelihood, which measures how likely it is to observe our given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Nashville Precipitation\n",
    "\n",
    "The dataset `nashville_precip.txt` contains [NOAA precipitation data for Nashville measured since 1871](http://bit.ly/nasvhville_precip_data). The gamma distribution is often a good fit to aggregated rainfall data, and will be our candidate distribution in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip = pd.read_table(\"data/nashville_precip.txt\", index_col=0, na_values='NA', delim_whitespace=True)\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip.hist(sharex=True, sharey=True, grid=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is recognixing what sort of distribution to fit our data to. A couple of observations:\n",
    "\n",
    "1. The data are skewed, with a longer tail to the right than to the left\n",
    "2. The data are positive-valued, since they are measuring rainfall\n",
    "3. The data are continuous\n",
    "\n",
    "There are a few possible choices, but one suitable alternative is the **gamma distribution**:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$x \\sim \\text{Gamma}(\\alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)}$$\n",
    "</div>\n",
    "\n",
    "![gamma](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Gamma_distribution_pdf.svg/500px-Gamma_distribution_pdf.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***method of moments*** simply assigns the empirical mean and variance to their theoretical counterparts, so that we can solve for the parameters.\n",
    "\n",
    "So, for the gamma distribution, the mean and variance are:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$ \\hat{\\mu} = \\bar{X} = \\alpha \\beta $$\n",
    "$$ \\hat{\\sigma}^2 = S^2 = \\alpha \\beta^2 $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we solve for these parameters, we can use a gamma distribution to describe our data:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$ \\alpha = \\frac{\\bar{X}^2}{S^2}, \\, \\beta = \\frac{S^2}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with the missing value in the October data. Given what we are trying to do, it is most sensible to fill in the missing value with the average of the available values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.fillna(value={'Oct': precip.Oct.mean()}, inplace=True)\n",
    "precip.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_mean = precip.mean()\n",
    "precip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_var = precip.var()\n",
    "precip_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mom = precip_mean ** 2 / precip_var\n",
    "beta_mom = precip_var / precip_mean\n",
    "alpha_mom, beta_mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `gamma.pdf` function in `scipy.stats.distributions` to plot the ditribtuions implied by the calculated alphas and betas. For example, here is January:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import gamma\n",
    "\n",
    "precip.Jan.hist(normed=True, bins=20)\n",
    "plt.plot(np.linspace(0, 10), gamma.pdf(np.linspace(0, 10), alpha_mom[0], beta_mom[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping over all months, we can create a grid of plots for the distribution of rainfall, using the gamma distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "axs = precip.hist(normed=True, figsize=(12, 8), sharex=True, sharey=True, bins=15, grid=False)\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    \n",
    "    # Get month\n",
    "    m = ax.get_title()\n",
    "    \n",
    "    # Plot fitted distribution\n",
    "    print(ax.get_xlim())\n",
    "    x = np.linspace(*ax.get_xlim())\n",
    "    ax.plot(x, gamma.pdf(x, alpha_mom[m], beta_mom[m]))\n",
    "    \n",
    "    # Annotate with parameter estimates\n",
    "    label = 'alpha = {0:.2f}\\nbeta = {1:.2f}'.format(alpha_mom[m], beta_mom[m])\n",
    "    ax.annotate(label, xy=(10, 0.2))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood\n",
    "==================\n",
    "\n",
    "**Maximum likelihood** (ML) fitting is usually more work than the method of moments, but it is preferred as the resulting estimator is known to have good theoretical properties. \n",
    "\n",
    "There is a ton of theory regarding ML. We will restrict ourselves to the mechanics here.\n",
    "\n",
    "Say we have some data $y = y_1,y_2,\\ldots,y_n$ that is distributed according to some distribution:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$Pr(Y_i=y_i | \\theta)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the rainfall data, if we are using a gamma distribution we need to maximize:\n",
    "\n",
    "$$\\begin{align}l(\\alpha,\\beta) &= \\sum_{i=1}^n \\log[\\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}] \\cr \n",
    "&= n[(\\alpha-1)\\overline{\\log(x)} - \\bar{x}\\beta + \\alpha\\log(\\beta) - \\log\\Gamma(\\alpha)]\\end{align}$$\n",
    "\n",
    "(*Its usually easier to work in the log scale*)\n",
    "\n",
    "where $n = 2012 − 1871 = 141$ and the bar indicates an average over all *i*. We choose $\\alpha$ and $\\beta$ to maximize $l(\\alpha,\\beta)$.\n",
    "\n",
    "Notice $l$ is infinite if any $x$ is zero. We do not have any zeros, but we do have an NA value for one of the October data, which we dealt with above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the MLE\n",
    "\n",
    "To find the maximum of any function, we typically take the *derivative* with respect to the variable to be maximized, set it to zero and solve for that variable. \n",
    "\n",
    "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\beta} = n\\left(\\frac{\\alpha}{\\beta} - \\bar{x}\\right) = 0$$\n",
    "\n",
    "Which can be solved as $\\beta = \\alpha/\\bar{x}$. However, plugging this into the derivative with respect to $\\alpha$ yields:\n",
    "\n",
    "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\alpha} = \\log(\\alpha) + \\overline{\\log(x)} - \\log(\\bar{x}) - \\frac{\\Gamma(\\alpha)'}{\\Gamma(\\alpha)} = 0$$\n",
    "\n",
    "This has no closed form solution. We must use ***numerical optimization***!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical optimization alogarithms take an initial \"guess\" at the solution, and iteratively improve the guess until it gets \"close enough\" to the answer.\n",
    "\n",
    "Here, we will use Newton-Raphson algorithm:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is available to us via SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Newton-Raphson algorithm, we need a function that returns a vector containing the **first and second derivatives** of the function with respect to the variable of interest. In our case, this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma\n",
    "\n",
    "dlgamma = lambda m, log_mean, mean_log: np.log(m) - psi(m) - log_mean + mean_log\n",
    "dl2gamma = lambda m, *args: 1./m - polygamma(1, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `log_mean` and `mean_log` are $\\log{\\bar{x}}$ and $\\overline{\\log(x)}$, respectively. `psi` and `polygamma` are complex functions of the Gamma function that result when you take first and second derivatives of that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "log_mean = precip.mean().apply(np.log)\n",
    "mean_log = precip.apply(np.log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha MLE for December\n",
    "alpha_mle = newton(dlgamma, 2, dl2gamma, args=(log_mean[-1], mean_log[-1]))\n",
    "alpha_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plug this back into the solution for beta:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\beta  = \\frac{\\alpha}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_mle = alpha_mle/precip.mean()[-1]\n",
    "beta_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dec = precip.Dec\n",
    "dec.hist(normed=True, bins=10, grid=False)\n",
    "x = np.linspace(0, dec.max())\n",
    "plt.plot(x, gamma.pdf(x, alpha_mom[-1], beta_mom[-1]), 'g-')\n",
    "plt.plot(x, gamma.pdf(x, alpha_mle, beta_mle), 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression models\n",
    "\n",
    "A general, primary goal of many statistical data analysis tasks is to relate the influence of one variable on another. For example, we may wish to know how different medical interventions influence the incidence or duration of disease, or perhaps a how baseball player's performance varies as a function of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    "plt.plot(x,y,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a model to characterize the relationship between $X$ and $Y$, recognizing that additional factors other than $X$ (the ones we have measured or are interested in) may influence the response variable $Y$.\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = f(x_i) + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f$ is some function, for example a linear function:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\epsilon_i$ accounts for the difference between the observed response $y_i$ and its prediction from the model $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$. This is sometimes referred to as **process uncertainty**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to select $\\beta_0, \\beta_1$ so that the difference between the predictions and the observations is zero, but this is not usually possible. Instead, we choose a reasonable criterion: ***the smallest sum of the squared differences between $\\hat{y}$ and $y$***.\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$R^2 = \\sum_i (y_i - [\\beta_0 + \\beta_1 x_i])^2 = \\sum_i \\epsilon_i^2 $$  \n",
    "</div>\n",
    "\n",
    "Squaring serves two purposes: (1) to prevent positive and negative values from cancelling each other out and (2) to strongly penalize large deviations. Whether the latter is a good thing or not depends on the goals of the analysis.\n",
    "\n",
    "In other words, we will select the parameters that minimize the squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss([0,1],x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "b0,b1 = fmin(ss, [0,1], args=(x,y))\n",
    "b0,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not restricted to a straight-line regression model; we can represent a curved relationship between our variables by introducing **polynomial** terms. For example, a cubic model:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss2 = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x - theta[2]*(x**2)) ** 2)\n",
    "b0,b1,b2 = fmin(ss2, [1,1,-1], args=(x,y))\n",
    "print (b0, b1,b2)\n",
    "plt.plot(x, y, 'ro')\n",
    "xvals = np.linspace(0, 10, 100)\n",
    "plt.plot(xvals, b0 + b1*xvals + b2*(xvals**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
